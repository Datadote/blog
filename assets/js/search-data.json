{
  
    
        "post0": {
            "title": "Fastbook_ch4_ans",
            "content": "How is a greyscale image represented on a computer? How about a color image? For greyscale, 1 number indicating range from white to black For color, 3 numbers indicating intensity of red, green, blue . | How are the files and folders in the MNIST_SAMPLE dataset structured? Why? Structured similar to imagenet Main folder -&gt; valid, train valid,train -&gt; separate folders for each category . | Explain how the “pixel similarity” approach to classifying digits works. Pixel similarity is where you find the ideal number by averaging all the images of the “number”, then you take the mean absolute difference or mean squared difference between your input and the ideal image . | What is a list comprehension? Create one now that selects odd numbers from a list and doubles them. List comprehension is a fast way to create a list from an iterator a = [x for x in range(10)]; [2*x for x in a if x%2 &gt; 0] . | What is a “rank 3 tensor”? a tensor with 3 dimensions . | What is the difference between tensor rank and shape? How do you get the rank from the shape? shape gives you information on how big each axis is. Rank is length(shape) . | What are RMSE and L1 norm? RMSE and L1 norm are methods to measure difference between two things. RMSE stands for root mean squared error. L1 norm is the mean absolute difference. . | How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop? Vectorize the calculations, and then use a gpu . | Create a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom right 4 numbers. a = (torch.tensor(range(9))*2).view(3,3); a[1:3, 1:3] . | What is broadcasting? broadcasting is when a math operation happens between two inputs with different shapes (usually 1 dimension), and the smaller shape automatically becomes the shape of the larger shape . | Are metrics generally calculated using the training set, or the validation set? Why? validation set in order to make sure model is generalizing and not overfitting . | What is SGD? SGD stands for stochastic gradient descent, and is a method to optimize neural network weights. . | Why does SGD use mini batches? Speeds up training, rather than going 1 example at a time . | What are the 7 steps in SGD for machine learning? Initialize parameters, forward prop, calculate loss, backprop, update weights, repeat, stop . | How do we initialize the weights in a model? Randomly. Although with certain activations, there are certain statistics we use with the random initialization. Eg ReLU -&gt; he intialization . | What is “loss”? loss is a measure the algorithm uses to optimize itself . | Why can’t we always use a high learning rate? a high learning rate could cause the loss to increase after each update . | What is a “gradient”? gradient is the slope of a function at a point . | Do you need to know how to calculate gradients yourself? If you use a framework, no. PyTorch does it for you . | Why can’t we use accuracy as a loss function? Accuracy is not fine-grain enough. Most gradients would be close to 0, and the updating would be slow . | Draw the sigmoid function. What is special about its shape? Goes between 0 and 1. 1/(1+exp(-x)). Crosses 0.5 at x=0 . | What is the difference between loss and metric? Loss is the measure the algorithm uses to evaluate performance. Metric is what humans use. . | What is the function to calculate new weights using a learning rate? w = w - lrparams.grad, or params.data -= lrparams.grad . | What does the DataLoader class do? dataloader takes a list of tuples of (x,y), and batches/shuffles them for your model . | Write pseudo-code showing the basic steps taken each epoch for SGD. forward prop, calc loss, backprop, update weight pred = model(x) loss = loss_func(pred, y) loss.backward() p -= p.grad*lr p.grad.zero_() . | Create a function which, if passed two arguments [1,2,3,4] and &#39;abcd&#39;, returns [(1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;c&#39;), (4, &#39;d&#39;)]. What is special about that output data structure? [x for x in zip([1,2,3,4], ‘abcd’)] . | What does view do in PyTorch? view does reshaping . | What are the “bias” parameters in a neural network? Why do we need them? bias parameters are randomly initialized parameters used in the forward prop equation. y = w*x + b. B lets the equation center itself. . | What does the @ operator do in python? @ = np.matmul = element-wise matrix multiplication . | What does the backward method do? backward calculates the gradients from the given point eg loss.backward() . | Why do we have to zero the gradients? so that the update is reflective of the current step, and not of previous steps loss.backward() accumulates gradients in each function . | What information do we have to pass to Learner? Learner(dls, net, metrics) Learner(dls, net, opt_func, loss_func, metrics) . | Show python or pseudo-code for the basic steps of a training loop. for i in range(epochs): for xb,yb in dl: calc_grad(xb, yb, model) p.data -= p.grad.data*lr p.grad.zero_() . | What is “ReLU”? Draw a plot of it for values from -2 to +2. relu(x) = max(x,0). Relu is 0 for x 0 or less, and = x for x &gt; 0 . | What is an “activation function”? activation function is a nonlinearity applied to the linear y=wx+b equation . | What’s the difference between F.relu and nn.ReLU? F.relu is the function. nn.Relu is a PyTorch module (layer) that does the same thing. . | The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more? We use more nonlinearities, which means more layers, for better performance. For the same performance, deeper networks tend to need less memory/calculations. . |",
            "url": "https://datadote.github.io/blog/2020/04/07/fastbook_ch4_ans.html",
            "relUrl": "/2020/04/07/fastbook_ch4_ans.html",
            "date": " • Apr 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastbook_ch3_ans",
            "content": "Does ethics provide a list of “right answers”? There is no list of right answers for ethics . | How can working with people of different backgrounds help when considering ethical questions? Provide different viewpoints on how an application will affect people . | What was the role of IBM in Nazi Germany? Why did the company participate as they did? Why did the workers participate? IBM provided equipment and technical assistance to classify prisoners. Company participated for money. Workers did not know better. . | What was the role of the first person jailed in the VW diesel scandal? Engineer who just did what manager told him to do. . | What was the problem with a database of suspected gang members maintained by California law enforcement officials? The database had many errors, but had no easy means of fixing the errors. The errors included 42 babies added to database then they were less than 1 year old. 28 of these babies were marked as admitting to being gang members. . | Why did YouTube’s recommendation algorithm recommend videos of partially clothed children to pedophiles, even although no employee at Google programmed this feature? The algorithm optimized itself to maximize viewer time. . | What are the problems with the centrality of metrics? Centrality of metrics, to the extreme, mean one metric is being optimized. Algorithms will try to optimize the metric without thinking about negative consequences. . | Why did Meetup.com not include gender in their recommendation system for tech meetups? More men than women were interested in tech meetups. If the recommendation system included gender, it might have increased this bias, and have a positive feedback loop. Recommend more to men, recommend less to women. . | What are the six types of bias in machine learning, according to Suresh and Guttag? Historical bias, representation bias, measurement bias, evaluation bias, aggregation bias, deployment bias . | Give two examples of historical race bias in the US Used car sales, black people offered higher initial prices. Craigslist rental-ads, Black name elicited fewer respones than a white name. . | Where are most images in Imagenet from? United States and other Western countries. Eg Great Britain, Itality, Canada, Australia, Spain. . | In the paper “Does Machine Learning Automate Moral Hazard and Error” why is sinusitis found to be predictive of a stroke? The data only represented people who had symptoms, went to the hospital, and got diagnosed with a stroke. Based on the data, the prediction was similar to prediction heavy utilization (propensity of people to seek cre) as well as the stroke. . | What is representation bias? When there is a clear imbalance, a model will find it and amplify it or maintain it. For example in occupations, models tend to predict females as nurses, and males as doctors. . | How are machines and people different, in terms of their use for making decisions? Machines can amplify bias with feedback loops very rapidly. Machines are implemented at scale without thinking of potential negative consequences, whereas people are usually screened more before being put into positions of power. . | Is disinformation the same as “fake news”? No, disinformation is making people not trust one another by intertwining real and fake news together. . | Why is disinformation through auto-generated text a particularly significant issue? Auto-generated disinformation scales a lot easier than human trolls. As a result, the amount of disinformation could exponentially increase without much resources. . | What are the five ethical lenses described by the Markkula Center? The Rights Approach: Which option best respects the rights of all who have a stake? The Justice Approach: Which option treats people equally or proportionately? The Utilitarian Approach: Which option will produce the most good and do the least harm? The Common Good Approach: Which option best serves the community as a whole, and not just some members? The Virtue Approach: Which option leads me to act as the sort of person I want to be? . | Where is policy an appropriate tool for addressing data ethics issues? If the data ethics issues are human rights issues, then the law is an appropriate tool. . |",
            "url": "https://datadote.github.io/blog/2020/04/06/fastbook_ch3_ans.html",
            "relUrl": "/2020/04/06/fastbook_ch3_ans.html",
            "date": " • Apr 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastbook_ch2_ans",
            "content": "Chapter 2 answers to questions . Provide an example of where the bear classification model might work poorly, due to structural or style differences to the training data. Night time where the lighting is darker and images are more grayscale . | Where do text models currently have a major deficiency? Text models can mimic text well, but don’t seem to understand what they are mimicing. Text models are not good at generating correct responses . | What are possible negative societal implications of text generation models? Biases can be built into these models. These text models could be used for online trolling or spreading misinfromation on social media . | In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? Add a human into the process to check the model’s predictions . | What kind of tabular data is deep learning particularly good at? Text, eg titles, reviews, comments . | What’s a key downside of directly using a deep learning model for recommendation systems? Deep learning models are predictors, not recommenders. So, the models look at your history, and find similar items. Rather than try to push your boundaries a little to see if you might like something new . | What are the steps of the Drivetrain approach? 1) Define objective 2) Figure out the levers 3) Get data 4) Use the data to model how the objective changes using the available levers . | How do the steps of the Drivetrain approach map to a recommendation system? The Drivetrain approach is a methodology to create actionable outcomes from data, and not just predictions. 1) Want more sales through recommendations 2) Rank the recommendations 3) Run experiments to see if new recommendations cause new sales 4) Compare new model with existing/baseline model to see if changes caused more sales from recommendations . | Create an image recognition model using data you curate, and deploy it on the web. Done . | What is DataLoaders? A dataloaders is a collection of dataloader objects from the FASTAI library A dataloader object contains information on how to retrieve the data (paths, names, labels, splits) . | What four things do we need to tell fastai to create DataLoaders? 1) What kind of data we are working with 2) How to get the list of items 3) How to label these images 4) How to create a validation set . | What does the splitter parameter to DataBlock do? Splitter tells the dataloaders function how to split the data into training and validation sets . | How do we ensure a random split always gives the same validation set? Pass a seed=### argument into the RandomSplitter. Use the same seed number each time . | What letters are often used to signify the independent and dependent variables? Independent = x Dependent = y . | What’s the difference between crop, pad, and squish resize approaches? When might you choose one over the other? Crop takes a centered crop of the image Pad adds 0’s to the perimeter of images if the images are too small (black borders) Squish squeezes an image into the specified size. Distorts image If the images are always centered, crop is ok. If you want the whole image without any changes, and have different sizes, then padding works. Use squish if you need faster compute, and the application actually has similar distortions. . | What is data augmentation? Why is it needed? Data augmentation is when you add transforms (change contrast, brightness, rotate, skew, flip, etc.) to images. Data augmentation is used when the training data is small, which seems to be the case for all image data sets. . | What is the difference between item_tfms and batch_tfms? Item_tfms does transformations on a per item basis, and is usually just used for resizing images. Item_tfms works before batch_tfms Batch_tfms applies transforms on a batch at a time. As a result, it assumes the inputs are all the same shape item_tfms and batch_tfms are arguments for the DataLoader class . | What is a confusion matrix? A confusion matrix is a table showing correct/wrong predictions, as well as false positives and false negatives. On the table, the y&gt;axis are the actual labels. On the x&gt;axis are the model predictions. Given a table x,y point, that point represents the number of times the actual label, and predicted label happened together. The diagonal of the table represents when the actual label and predicted label were the same. . | What does export save? Export saves a ‘.pkl’ file. This file contains the model architecture, parameters, and also the definition of how to create the dataloaders . | What is it called when we use a model for getting predictions, instead of training? Inference . | What are IPython widgets? Software that allows you to use JavaScript and Python in a web browser . | When might you want to use CPU for deployment? When might GPU be better? Use CPU for deployment when the user experience is adequate. CPU is generally cheaper. May want a GPU if you get enough traffic, and are able to batch together jobs . | What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? Deploying to your server means you have to maintain the hardware, and also the scaling burden is on you. On a phone, it is horizontal scaling and the client “takes” the scaling cost . | What are 3 examples of problems that could occur when rolling out a bear warning system in practice? 1) Out of domain data, 2) Domain shift Examples of “out of domain data” are low resolution photos, night time photos, latency between prediction and response to user . | What is “out of domain data”? Input data that has a distribution a lot different than the training data distribution . | What is “domain shift”? Domain shift is when your input data distribution changes, and moves away from the original training data distribution . | What are the 3 steps in the deployment process? 1) Manual process, run models in parallel and check outputs by hand 2) Limited scope deployment, try the model in the wild for a limited time/area and supervise the performance 3) Gradual expansion, expand the scope. This will require a good reporting system in order to get debugging information if things go wrong. For example, out of domain data or domain shift. . | For a project you’re interested in applying deep learning to, consider the thought experiment “what would happen if it went really, really well?” Done . | Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you’re interested in. Done. Thinking about an invasive flower classifier. . | Furthur research . Consider how the Drivetrain approach maps to a project or problem you’re interested in. I think the approach let’s you understand how different parts interact . | When might it be best to avoid certain types of data augmentation? When the data augmentation transforms images to look like another image label. For example, on MNIST, you do not want to do a horizontal flip because a b and d could look the same. Another example are numbers where you do not want to flip vertically because a 6 and a 9 could look the same . |",
            "url": "https://datadote.github.io/blog/2020/03/29/fastbook_ch2_ans.html",
            "relUrl": "/2020/03/29/fastbook_ch2_ans.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastbook_ch1_ans",
            "content": "Chapter 1 Answers to questions . 1) Do you need these for deep learning? Lots of math T / F Lots of data T / F Lots of expensive computers T / F A PhD T / F . FFFF . 2) Name five areas where deep learning is now the best in the world. . Vision, NLP, robotics, image generation, playing games, recommendation systems . 3) What was the name of the first device that was based on the principle of the artificial neuron? . The Mark I Perceptron . 4) Based on the book of the same name, what are the requirements for “Parallel Distributed Processing”? . Needs the following: set of processing units, a state of activation, an output function for each unit, a pattern of connectivity among units, a propagation rule for propagating patterns of activity through the network of connectivities, an activation rule to combine inputs, a learning rule where patterns of connectivity are modified by experience, an environment within which the system must operate . ?? 5) What were the two theoretical misunderstandings that held back the field of neural networks? . Marvin Minsky showing 1 layer of NN couldn’t perform some math functions (eg XOR) . 6) What is a GPU? . Graphics processing unit, good for matrix multiplication . 7) Open a notebook and execute a cell containing: 1+1. What happens? . Prints out 2 . 8) Follow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen . Done . 9) Complete the Jupyter Notebook online appendix. . Done . 10) Why is it hard to use a traditional computer program to recognize images in a photo? . You have to write out rules explicitly. To recognize images would take an unreasonable amount of rules. . 11) What did Samuel mean by “Weight Assignment”? . Each processing unit has a weight associated with it. This weight is updated through training. . 12) What term do we normally use in deep learning for what Samuel called “Weights”? . model parameters . 13) Draw a picture that summarizes Arthur Samuel’s view of a machine learning model . Done . 14) Why is it hard to understand why a deep learning model makes a particular prediction? . Model learns from data, and does not explain the rules explicitly . 15) What is the name of the theorem that a neural network can solve any mathematical problem to any level of accuracy? . Universal Approximation Theorm . 16) What do you need in order to train a model? . Data . 17) How could a feedback loop impact the rollout of a predictive policing model? . Positive feedback increasing a bias. Examples include predictive models for arrests or collaborative filtering for youtube videos. . 18) Do we always have to use 224x224 pixel images with the cat recognition model? . Historical reasons on imagenet . 19) What is the difference between classification and regression? . Classification has discrete outputs. Linear regression has continuous outputs . 20) What is a validation set? What is a test set? Why do we need them? . Validation set is a portion of the dataset set aside for the human to evaluate performance. It is not used for training, but for evaluation of the training . 21) What will fastai do if you don’t provide a validation set? . Fastai will automatically make a validation set based off 20% of your training data . 22) Can we always use a random sample for a validation set? Why or why not? . No, because there is a chance the validation set does not represent the actual use case. An example is time series forecasting, we want to predict the future given the past. If our training data contains data in the future of the validation set, then there is data leakage. . 23) What is overfitting? Provide an example. . Overfitting is when the model memorizes pictures, and does not learn general features. An indicator of overfitting is when there is a large difference between the training and validation loss. An example might be a car dataset where the model sees a lot of Toyota cars, but can’t tell Honda cars are cars as well. Maybe the model learned to associate cars with the Toyota logo. . 24) What is a metric? How does it differ to “loss”? . A metric is a performance indicator for humans to understand. A loss is a performance indicator for models to use in order to train, and adjust weights. . 25) How can pretrained models help? . Pretrained models help by leveraging knowledge learned on other tasks. An example is image classification. Models trained on ImageNet learn low level features such as edges, textures, gradients, and can transfer this learned knowledge. Small datasets might not have enough data to let untrained models learn these low level features. . 26) What is the “head” of a model? . The head of a model is a smaller neural network added to the last layer of a pretrained model. This head is trainable, and fine-tuned to the applicable dataset. . 27) What kinds of features do the early layers of a CNN find? How about the later layers? . Early layers of a CNN learn low level features such as edges, lines, textures, gradients. Later layers combine the low level features, and contain more complex features such as shapes, text, faces. . 28) Are image models only useful for photos? . No, image models can used for other types of data as well. The caveat is the other types of data must be converted into images at some point. One example is speech. Speech spectrograms can be turned into pictures, and then fed into a model. . 29) What is an “architecture”? . An architecture is the skeleton of a neural network. This architecture includes the number of layers, number of weights, pattern of connections, and activation functions. . 30) What is segmentation? . Segmentation is the act of semantically classifying every pixel of an image. . 31) What is y_range used for? When do we need it? . y_range is used to indicate continuous variables for tabular data. . 32) What are “hyperparameters”? . Hyperparameters are parameters set by the human, and not learned by the model training. Examples of hyperparameters are the learning rate, activation functions, number of layers, number of weights to use. An example of not a parameter, but not a hyperparameter, are the weights of the neural network neurons. These weights are learned through training. . 33) What’s the best way to avoid failures when using AI in an organization? . Best way to avoid failures is to understand the importance of validation and test sets. Validation sets are used to verify the training performance. If the validation sets are used too often, there is a chance to overfit to the validation set. As a result, a test set is used to “double check” a model’s performance. A test set should be used seldomly to avoid overfitting to the test set. .",
            "url": "https://datadote.github.io/blog/2020/03/25/fastbook_ch1_ans.html",
            "relUrl": "/2020/03/25/fastbook_ch1_ans.html",
            "date": " • Mar 25, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Title",
            "content": "Tutorial: Transfer Learning with EfficientNet . 1) Import Libraries . This tutorial is tested with tensorflow 2.1 gpu | EfficientNetB0 comes from keras_applications.efficientnet | . from pathlib import Path from tensorflow.keras.layers import Flatten, Dense, GlobalAveragePooling2D from tensorflow.keras.models import Model from keras_preprocessing.image import ImageDataGenerator from keras_applications.efficientnet import EfficientNetB0 import tensorflow as tf import tensorflow.keras.backend as K K.set_image_data_format(&#39;channels_last&#39;) import matplotlib.pyplot as plt %matplotlib inline . 2) Download dataset, and set directory paths . Image dataset comes from https://github.com/fastai/imagenette | The 320 px dataset is used here | Download link: https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz | . current_dir = Path().absolute() train_dir = current_dir/&#39;data/imagenette2-320/train&#39; val_dir = current_dir/&#39;data/imagenette2-320/val&#39; . 3) Create image data generators . If GPU runs out of memory, consider changing batch_size (e.g. batch_size = 16) | . batch_size=32 num_classes=10 input_shape = (224, 224, 3) # Normalize images train_datagen = ImageDataGenerator(rescale = 1./255) val_datagen = ImageDataGenerator(rescale = 1./255) train_it = train_datagen.flow_from_directory(train_dir, target_size=input_shape[:2], shuffle=True, class_mode=&#39;categorical&#39;, batch_size=batch_size) val_it = val_datagen.flow_from_directory(val_dir, target_size=input_shape[:2], shuffle=True, class_mode=&#39;categorical&#39;, batch_size=batch_size) . Found 9469 images belonging to 10 classes. Found 3925 images belonging to 10 classes. . 4) Create base model from EfficientNetB0 . We will fine tune EfficientNet to our dataset -&gt; include_top = False | . kwargs = {&#39;backend&#39;: tf.keras.backend, &#39;layers&#39;: tf.keras.layers, &#39;models&#39;: tf.keras.models, &#39;utils&#39;: tf.keras.utils} base_model = EfficientNetB0(include_top=False, weights=&#39;imagenet&#39;, input_shape=input_shape, **kwargs) . 5) Add some top layers, make base model untrainable, compile . x = base_model.output x = GlobalAveragePooling2D()(x) x = Flatten()(x) predictions = Dense(num_classes, activation=&#39;softmax&#39;)(x) new_model = Model(inputs=base_model.input, outputs=predictions) for layer in base_model.layers: layer.trainable = False new_model.compile(optimizer=&#39;Adam&#39;, loss = &#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) new_model.summary() . Model: &#34;model&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 224, 224, 3) 0 __________________________________________________________________________________________________ stem_conv_pad (ZeroPadding2D) (None, 225, 225, 3) 0 input_1[0][0] __________________________________________________________________________________________________ stem_conv (Conv2D) (None, 112, 112, 32) 864 stem_conv_pad[0][0] __________________________________________________________________________________________________ stem_bn (BatchNormalization) (None, 112, 112, 32) 128 stem_conv[0][0] __________________________________________________________________________________________________ stem_activation (Activation) (None, 112, 112, 32) 0 stem_bn[0][0] __________________________________________________________________________________________________ block1a_dwconv (DepthwiseConv2D (None, 112, 112, 32) 288 stem_activation[0][0] __________________________________________________________________________________________________ block1a_bn (BatchNormalization) (None, 112, 112, 32) 128 block1a_dwconv[0][0] __________________________________________________________________________________________________ block1a_activation (Activation) (None, 112, 112, 32) 0 block1a_bn[0][0] __________________________________________________________________________________________________ block1a_se_squeeze (GlobalAvera (None, 32) 0 block1a_activation[0][0] __________________________________________________________________________________________________ block1a_se_reshape (Reshape) (None, 1, 1, 32) 0 block1a_se_squeeze[0][0] __________________________________________________________________________________________________ block1a_se_reduce (Conv2D) (None, 1, 1, 8) 264 block1a_se_reshape[0][0] __________________________________________________________________________________________________ block1a_se_expand (Conv2D) (None, 1, 1, 32) 288 block1a_se_reduce[0][0] __________________________________________________________________________________________________ block1a_se_excite (Multiply) (None, 112, 112, 32) 0 block1a_activation[0][0] block1a_se_expand[0][0] __________________________________________________________________________________________________ block1a_project_conv (Conv2D) (None, 112, 112, 16) 512 block1a_se_excite[0][0] __________________________________________________________________________________________________ block1a_project_bn (BatchNormal (None, 112, 112, 16) 64 block1a_project_conv[0][0] __________________________________________________________________________________________________ block2a_expand_conv (Conv2D) (None, 112, 112, 96) 1536 block1a_project_bn[0][0] __________________________________________________________________________________________________ block2a_expand_bn (BatchNormali (None, 112, 112, 96) 384 block2a_expand_conv[0][0] __________________________________________________________________________________________________ block2a_expand_activation (Acti (None, 112, 112, 96) 0 block2a_expand_bn[0][0] __________________________________________________________________________________________________ block2a_dwconv_pad (ZeroPadding (None, 113, 113, 96) 0 block2a_expand_activation[0][0] __________________________________________________________________________________________________ block2a_dwconv (DepthwiseConv2D (None, 56, 56, 96) 864 block2a_dwconv_pad[0][0] __________________________________________________________________________________________________ block2a_bn (BatchNormalization) (None, 56, 56, 96) 384 block2a_dwconv[0][0] __________________________________________________________________________________________________ block2a_activation (Activation) (None, 56, 56, 96) 0 block2a_bn[0][0] __________________________________________________________________________________________________ block2a_se_squeeze (GlobalAvera (None, 96) 0 block2a_activation[0][0] __________________________________________________________________________________________________ block2a_se_reshape (Reshape) (None, 1, 1, 96) 0 block2a_se_squeeze[0][0] __________________________________________________________________________________________________ block2a_se_reduce (Conv2D) (None, 1, 1, 4) 388 block2a_se_reshape[0][0] __________________________________________________________________________________________________ block2a_se_expand (Conv2D) (None, 1, 1, 96) 480 block2a_se_reduce[0][0] __________________________________________________________________________________________________ block2a_se_excite (Multiply) (None, 56, 56, 96) 0 block2a_activation[0][0] block2a_se_expand[0][0] __________________________________________________________________________________________________ block2a_project_conv (Conv2D) (None, 56, 56, 24) 2304 block2a_se_excite[0][0] __________________________________________________________________________________________________ block2a_project_bn (BatchNormal (None, 56, 56, 24) 96 block2a_project_conv[0][0] __________________________________________________________________________________________________ block2b_expand_conv (Conv2D) (None, 56, 56, 144) 3456 block2a_project_bn[0][0] __________________________________________________________________________________________________ block2b_expand_bn (BatchNormali (None, 56, 56, 144) 576 block2b_expand_conv[0][0] __________________________________________________________________________________________________ block2b_expand_activation (Acti (None, 56, 56, 144) 0 block2b_expand_bn[0][0] __________________________________________________________________________________________________ block2b_dwconv (DepthwiseConv2D (None, 56, 56, 144) 1296 block2b_expand_activation[0][0] __________________________________________________________________________________________________ block2b_bn (BatchNormalization) (None, 56, 56, 144) 576 block2b_dwconv[0][0] __________________________________________________________________________________________________ block2b_activation (Activation) (None, 56, 56, 144) 0 block2b_bn[0][0] __________________________________________________________________________________________________ block2b_se_squeeze (GlobalAvera (None, 144) 0 block2b_activation[0][0] __________________________________________________________________________________________________ block2b_se_reshape (Reshape) (None, 1, 1, 144) 0 block2b_se_squeeze[0][0] __________________________________________________________________________________________________ block2b_se_reduce (Conv2D) (None, 1, 1, 6) 870 block2b_se_reshape[0][0] __________________________________________________________________________________________________ block2b_se_expand (Conv2D) (None, 1, 1, 144) 1008 block2b_se_reduce[0][0] __________________________________________________________________________________________________ block2b_se_excite (Multiply) (None, 56, 56, 144) 0 block2b_activation[0][0] block2b_se_expand[0][0] __________________________________________________________________________________________________ block2b_project_conv (Conv2D) (None, 56, 56, 24) 3456 block2b_se_excite[0][0] __________________________________________________________________________________________________ block2b_project_bn (BatchNormal (None, 56, 56, 24) 96 block2b_project_conv[0][0] __________________________________________________________________________________________________ block2b_drop (Dropout) (None, 56, 56, 24) 0 block2b_project_bn[0][0] __________________________________________________________________________________________________ block2b_add (Add) (None, 56, 56, 24) 0 block2b_drop[0][0] block2a_project_bn[0][0] __________________________________________________________________________________________________ block3a_expand_conv (Conv2D) (None, 56, 56, 144) 3456 block2b_add[0][0] __________________________________________________________________________________________________ block3a_expand_bn (BatchNormali (None, 56, 56, 144) 576 block3a_expand_conv[0][0] __________________________________________________________________________________________________ block3a_expand_activation (Acti (None, 56, 56, 144) 0 block3a_expand_bn[0][0] __________________________________________________________________________________________________ block3a_dwconv_pad (ZeroPadding (None, 59, 59, 144) 0 block3a_expand_activation[0][0] __________________________________________________________________________________________________ block3a_dwconv (DepthwiseConv2D (None, 28, 28, 144) 3600 block3a_dwconv_pad[0][0] __________________________________________________________________________________________________ block3a_bn (BatchNormalization) (None, 28, 28, 144) 576 block3a_dwconv[0][0] __________________________________________________________________________________________________ block3a_activation (Activation) (None, 28, 28, 144) 0 block3a_bn[0][0] __________________________________________________________________________________________________ block3a_se_squeeze (GlobalAvera (None, 144) 0 block3a_activation[0][0] __________________________________________________________________________________________________ block3a_se_reshape (Reshape) (None, 1, 1, 144) 0 block3a_se_squeeze[0][0] __________________________________________________________________________________________________ block3a_se_reduce (Conv2D) (None, 1, 1, 6) 870 block3a_se_reshape[0][0] __________________________________________________________________________________________________ block3a_se_expand (Conv2D) (None, 1, 1, 144) 1008 block3a_se_reduce[0][0] __________________________________________________________________________________________________ block3a_se_excite (Multiply) (None, 28, 28, 144) 0 block3a_activation[0][0] block3a_se_expand[0][0] __________________________________________________________________________________________________ block3a_project_conv (Conv2D) (None, 28, 28, 40) 5760 block3a_se_excite[0][0] __________________________________________________________________________________________________ block3a_project_bn (BatchNormal (None, 28, 28, 40) 160 block3a_project_conv[0][0] __________________________________________________________________________________________________ block3b_expand_conv (Conv2D) (None, 28, 28, 240) 9600 block3a_project_bn[0][0] __________________________________________________________________________________________________ block3b_expand_bn (BatchNormali (None, 28, 28, 240) 960 block3b_expand_conv[0][0] __________________________________________________________________________________________________ block3b_expand_activation (Acti (None, 28, 28, 240) 0 block3b_expand_bn[0][0] __________________________________________________________________________________________________ block3b_dwconv (DepthwiseConv2D (None, 28, 28, 240) 6000 block3b_expand_activation[0][0] __________________________________________________________________________________________________ block3b_bn (BatchNormalization) (None, 28, 28, 240) 960 block3b_dwconv[0][0] __________________________________________________________________________________________________ block3b_activation (Activation) (None, 28, 28, 240) 0 block3b_bn[0][0] __________________________________________________________________________________________________ block3b_se_squeeze (GlobalAvera (None, 240) 0 block3b_activation[0][0] __________________________________________________________________________________________________ block3b_se_reshape (Reshape) (None, 1, 1, 240) 0 block3b_se_squeeze[0][0] __________________________________________________________________________________________________ block3b_se_reduce (Conv2D) (None, 1, 1, 10) 2410 block3b_se_reshape[0][0] __________________________________________________________________________________________________ block3b_se_expand (Conv2D) (None, 1, 1, 240) 2640 block3b_se_reduce[0][0] __________________________________________________________________________________________________ block3b_se_excite (Multiply) (None, 28, 28, 240) 0 block3b_activation[0][0] block3b_se_expand[0][0] __________________________________________________________________________________________________ block3b_project_conv (Conv2D) (None, 28, 28, 40) 9600 block3b_se_excite[0][0] __________________________________________________________________________________________________ block3b_project_bn (BatchNormal (None, 28, 28, 40) 160 block3b_project_conv[0][0] __________________________________________________________________________________________________ block3b_drop (Dropout) (None, 28, 28, 40) 0 block3b_project_bn[0][0] __________________________________________________________________________________________________ block3b_add (Add) (None, 28, 28, 40) 0 block3b_drop[0][0] block3a_project_bn[0][0] __________________________________________________________________________________________________ block4a_expand_conv (Conv2D) (None, 28, 28, 240) 9600 block3b_add[0][0] __________________________________________________________________________________________________ block4a_expand_bn (BatchNormali (None, 28, 28, 240) 960 block4a_expand_conv[0][0] __________________________________________________________________________________________________ block4a_expand_activation (Acti (None, 28, 28, 240) 0 block4a_expand_bn[0][0] __________________________________________________________________________________________________ block4a_dwconv_pad (ZeroPadding (None, 29, 29, 240) 0 block4a_expand_activation[0][0] __________________________________________________________________________________________________ block4a_dwconv (DepthwiseConv2D (None, 14, 14, 240) 2160 block4a_dwconv_pad[0][0] __________________________________________________________________________________________________ block4a_bn (BatchNormalization) (None, 14, 14, 240) 960 block4a_dwconv[0][0] __________________________________________________________________________________________________ block4a_activation (Activation) (None, 14, 14, 240) 0 block4a_bn[0][0] __________________________________________________________________________________________________ block4a_se_squeeze (GlobalAvera (None, 240) 0 block4a_activation[0][0] __________________________________________________________________________________________________ block4a_se_reshape (Reshape) (None, 1, 1, 240) 0 block4a_se_squeeze[0][0] __________________________________________________________________________________________________ block4a_se_reduce (Conv2D) (None, 1, 1, 10) 2410 block4a_se_reshape[0][0] __________________________________________________________________________________________________ block4a_se_expand (Conv2D) (None, 1, 1, 240) 2640 block4a_se_reduce[0][0] __________________________________________________________________________________________________ block4a_se_excite (Multiply) (None, 14, 14, 240) 0 block4a_activation[0][0] block4a_se_expand[0][0] __________________________________________________________________________________________________ block4a_project_conv (Conv2D) (None, 14, 14, 80) 19200 block4a_se_excite[0][0] __________________________________________________________________________________________________ block4a_project_bn (BatchNormal (None, 14, 14, 80) 320 block4a_project_conv[0][0] __________________________________________________________________________________________________ block4b_expand_conv (Conv2D) (None, 14, 14, 480) 38400 block4a_project_bn[0][0] __________________________________________________________________________________________________ block4b_expand_bn (BatchNormali (None, 14, 14, 480) 1920 block4b_expand_conv[0][0] __________________________________________________________________________________________________ block4b_expand_activation (Acti (None, 14, 14, 480) 0 block4b_expand_bn[0][0] __________________________________________________________________________________________________ block4b_dwconv (DepthwiseConv2D (None, 14, 14, 480) 4320 block4b_expand_activation[0][0] __________________________________________________________________________________________________ block4b_bn (BatchNormalization) (None, 14, 14, 480) 1920 block4b_dwconv[0][0] __________________________________________________________________________________________________ block4b_activation (Activation) (None, 14, 14, 480) 0 block4b_bn[0][0] __________________________________________________________________________________________________ block4b_se_squeeze (GlobalAvera (None, 480) 0 block4b_activation[0][0] __________________________________________________________________________________________________ block4b_se_reshape (Reshape) (None, 1, 1, 480) 0 block4b_se_squeeze[0][0] __________________________________________________________________________________________________ block4b_se_reduce (Conv2D) (None, 1, 1, 20) 9620 block4b_se_reshape[0][0] __________________________________________________________________________________________________ block4b_se_expand (Conv2D) (None, 1, 1, 480) 10080 block4b_se_reduce[0][0] __________________________________________________________________________________________________ block4b_se_excite (Multiply) (None, 14, 14, 480) 0 block4b_activation[0][0] block4b_se_expand[0][0] __________________________________________________________________________________________________ block4b_project_conv (Conv2D) (None, 14, 14, 80) 38400 block4b_se_excite[0][0] __________________________________________________________________________________________________ block4b_project_bn (BatchNormal (None, 14, 14, 80) 320 block4b_project_conv[0][0] __________________________________________________________________________________________________ block4b_drop (Dropout) (None, 14, 14, 80) 0 block4b_project_bn[0][0] __________________________________________________________________________________________________ block4b_add (Add) (None, 14, 14, 80) 0 block4b_drop[0][0] block4a_project_bn[0][0] __________________________________________________________________________________________________ block4c_expand_conv (Conv2D) (None, 14, 14, 480) 38400 block4b_add[0][0] __________________________________________________________________________________________________ block4c_expand_bn (BatchNormali (None, 14, 14, 480) 1920 block4c_expand_conv[0][0] __________________________________________________________________________________________________ block4c_expand_activation (Acti (None, 14, 14, 480) 0 block4c_expand_bn[0][0] __________________________________________________________________________________________________ block4c_dwconv (DepthwiseConv2D (None, 14, 14, 480) 4320 block4c_expand_activation[0][0] __________________________________________________________________________________________________ block4c_bn (BatchNormalization) (None, 14, 14, 480) 1920 block4c_dwconv[0][0] __________________________________________________________________________________________________ block4c_activation (Activation) (None, 14, 14, 480) 0 block4c_bn[0][0] __________________________________________________________________________________________________ block4c_se_squeeze (GlobalAvera (None, 480) 0 block4c_activation[0][0] __________________________________________________________________________________________________ block4c_se_reshape (Reshape) (None, 1, 1, 480) 0 block4c_se_squeeze[0][0] __________________________________________________________________________________________________ block4c_se_reduce (Conv2D) (None, 1, 1, 20) 9620 block4c_se_reshape[0][0] __________________________________________________________________________________________________ block4c_se_expand (Conv2D) (None, 1, 1, 480) 10080 block4c_se_reduce[0][0] __________________________________________________________________________________________________ block4c_se_excite (Multiply) (None, 14, 14, 480) 0 block4c_activation[0][0] block4c_se_expand[0][0] __________________________________________________________________________________________________ block4c_project_conv (Conv2D) (None, 14, 14, 80) 38400 block4c_se_excite[0][0] __________________________________________________________________________________________________ block4c_project_bn (BatchNormal (None, 14, 14, 80) 320 block4c_project_conv[0][0] __________________________________________________________________________________________________ block4c_drop (Dropout) (None, 14, 14, 80) 0 block4c_project_bn[0][0] __________________________________________________________________________________________________ block4c_add (Add) (None, 14, 14, 80) 0 block4c_drop[0][0] block4b_add[0][0] __________________________________________________________________________________________________ block5a_expand_conv (Conv2D) (None, 14, 14, 480) 38400 block4c_add[0][0] __________________________________________________________________________________________________ block5a_expand_bn (BatchNormali (None, 14, 14, 480) 1920 block5a_expand_conv[0][0] __________________________________________________________________________________________________ block5a_expand_activation (Acti (None, 14, 14, 480) 0 block5a_expand_bn[0][0] __________________________________________________________________________________________________ block5a_dwconv (DepthwiseConv2D (None, 14, 14, 480) 12000 block5a_expand_activation[0][0] __________________________________________________________________________________________________ block5a_bn (BatchNormalization) (None, 14, 14, 480) 1920 block5a_dwconv[0][0] __________________________________________________________________________________________________ block5a_activation (Activation) (None, 14, 14, 480) 0 block5a_bn[0][0] __________________________________________________________________________________________________ block5a_se_squeeze (GlobalAvera (None, 480) 0 block5a_activation[0][0] __________________________________________________________________________________________________ block5a_se_reshape (Reshape) (None, 1, 1, 480) 0 block5a_se_squeeze[0][0] __________________________________________________________________________________________________ block5a_se_reduce (Conv2D) (None, 1, 1, 20) 9620 block5a_se_reshape[0][0] __________________________________________________________________________________________________ block5a_se_expand (Conv2D) (None, 1, 1, 480) 10080 block5a_se_reduce[0][0] __________________________________________________________________________________________________ block5a_se_excite (Multiply) (None, 14, 14, 480) 0 block5a_activation[0][0] block5a_se_expand[0][0] __________________________________________________________________________________________________ block5a_project_conv (Conv2D) (None, 14, 14, 112) 53760 block5a_se_excite[0][0] __________________________________________________________________________________________________ block5a_project_bn (BatchNormal (None, 14, 14, 112) 448 block5a_project_conv[0][0] __________________________________________________________________________________________________ block5b_expand_conv (Conv2D) (None, 14, 14, 672) 75264 block5a_project_bn[0][0] __________________________________________________________________________________________________ block5b_expand_bn (BatchNormali (None, 14, 14, 672) 2688 block5b_expand_conv[0][0] __________________________________________________________________________________________________ block5b_expand_activation (Acti (None, 14, 14, 672) 0 block5b_expand_bn[0][0] __________________________________________________________________________________________________ block5b_dwconv (DepthwiseConv2D (None, 14, 14, 672) 16800 block5b_expand_activation[0][0] __________________________________________________________________________________________________ block5b_bn (BatchNormalization) (None, 14, 14, 672) 2688 block5b_dwconv[0][0] __________________________________________________________________________________________________ block5b_activation (Activation) (None, 14, 14, 672) 0 block5b_bn[0][0] __________________________________________________________________________________________________ block5b_se_squeeze (GlobalAvera (None, 672) 0 block5b_activation[0][0] __________________________________________________________________________________________________ block5b_se_reshape (Reshape) (None, 1, 1, 672) 0 block5b_se_squeeze[0][0] __________________________________________________________________________________________________ block5b_se_reduce (Conv2D) (None, 1, 1, 28) 18844 block5b_se_reshape[0][0] __________________________________________________________________________________________________ block5b_se_expand (Conv2D) (None, 1, 1, 672) 19488 block5b_se_reduce[0][0] __________________________________________________________________________________________________ block5b_se_excite (Multiply) (None, 14, 14, 672) 0 block5b_activation[0][0] block5b_se_expand[0][0] __________________________________________________________________________________________________ block5b_project_conv (Conv2D) (None, 14, 14, 112) 75264 block5b_se_excite[0][0] __________________________________________________________________________________________________ block5b_project_bn (BatchNormal (None, 14, 14, 112) 448 block5b_project_conv[0][0] __________________________________________________________________________________________________ block5b_drop (Dropout) (None, 14, 14, 112) 0 block5b_project_bn[0][0] __________________________________________________________________________________________________ block5b_add (Add) (None, 14, 14, 112) 0 block5b_drop[0][0] block5a_project_bn[0][0] __________________________________________________________________________________________________ block5c_expand_conv (Conv2D) (None, 14, 14, 672) 75264 block5b_add[0][0] __________________________________________________________________________________________________ block5c_expand_bn (BatchNormali (None, 14, 14, 672) 2688 block5c_expand_conv[0][0] __________________________________________________________________________________________________ block5c_expand_activation (Acti (None, 14, 14, 672) 0 block5c_expand_bn[0][0] __________________________________________________________________________________________________ block5c_dwconv (DepthwiseConv2D (None, 14, 14, 672) 16800 block5c_expand_activation[0][0] __________________________________________________________________________________________________ block5c_bn (BatchNormalization) (None, 14, 14, 672) 2688 block5c_dwconv[0][0] __________________________________________________________________________________________________ block5c_activation (Activation) (None, 14, 14, 672) 0 block5c_bn[0][0] __________________________________________________________________________________________________ block5c_se_squeeze (GlobalAvera (None, 672) 0 block5c_activation[0][0] __________________________________________________________________________________________________ block5c_se_reshape (Reshape) (None, 1, 1, 672) 0 block5c_se_squeeze[0][0] __________________________________________________________________________________________________ block5c_se_reduce (Conv2D) (None, 1, 1, 28) 18844 block5c_se_reshape[0][0] __________________________________________________________________________________________________ block5c_se_expand (Conv2D) (None, 1, 1, 672) 19488 block5c_se_reduce[0][0] __________________________________________________________________________________________________ block5c_se_excite (Multiply) (None, 14, 14, 672) 0 block5c_activation[0][0] block5c_se_expand[0][0] __________________________________________________________________________________________________ block5c_project_conv (Conv2D) (None, 14, 14, 112) 75264 block5c_se_excite[0][0] __________________________________________________________________________________________________ block5c_project_bn (BatchNormal (None, 14, 14, 112) 448 block5c_project_conv[0][0] __________________________________________________________________________________________________ block5c_drop (Dropout) (None, 14, 14, 112) 0 block5c_project_bn[0][0] __________________________________________________________________________________________________ block5c_add (Add) (None, 14, 14, 112) 0 block5c_drop[0][0] block5b_add[0][0] __________________________________________________________________________________________________ block6a_expand_conv (Conv2D) (None, 14, 14, 672) 75264 block5c_add[0][0] __________________________________________________________________________________________________ block6a_expand_bn (BatchNormali (None, 14, 14, 672) 2688 block6a_expand_conv[0][0] __________________________________________________________________________________________________ block6a_expand_activation (Acti (None, 14, 14, 672) 0 block6a_expand_bn[0][0] __________________________________________________________________________________________________ block6a_dwconv_pad (ZeroPadding (None, 17, 17, 672) 0 block6a_expand_activation[0][0] __________________________________________________________________________________________________ block6a_dwconv (DepthwiseConv2D (None, 7, 7, 672) 16800 block6a_dwconv_pad[0][0] __________________________________________________________________________________________________ block6a_bn (BatchNormalization) (None, 7, 7, 672) 2688 block6a_dwconv[0][0] __________________________________________________________________________________________________ block6a_activation (Activation) (None, 7, 7, 672) 0 block6a_bn[0][0] __________________________________________________________________________________________________ block6a_se_squeeze (GlobalAvera (None, 672) 0 block6a_activation[0][0] __________________________________________________________________________________________________ block6a_se_reshape (Reshape) (None, 1, 1, 672) 0 block6a_se_squeeze[0][0] __________________________________________________________________________________________________ block6a_se_reduce (Conv2D) (None, 1, 1, 28) 18844 block6a_se_reshape[0][0] __________________________________________________________________________________________________ block6a_se_expand (Conv2D) (None, 1, 1, 672) 19488 block6a_se_reduce[0][0] __________________________________________________________________________________________________ block6a_se_excite (Multiply) (None, 7, 7, 672) 0 block6a_activation[0][0] block6a_se_expand[0][0] __________________________________________________________________________________________________ block6a_project_conv (Conv2D) (None, 7, 7, 192) 129024 block6a_se_excite[0][0] __________________________________________________________________________________________________ block6a_project_bn (BatchNormal (None, 7, 7, 192) 768 block6a_project_conv[0][0] __________________________________________________________________________________________________ block6b_expand_conv (Conv2D) (None, 7, 7, 1152) 221184 block6a_project_bn[0][0] __________________________________________________________________________________________________ block6b_expand_bn (BatchNormali (None, 7, 7, 1152) 4608 block6b_expand_conv[0][0] __________________________________________________________________________________________________ block6b_expand_activation (Acti (None, 7, 7, 1152) 0 block6b_expand_bn[0][0] __________________________________________________________________________________________________ block6b_dwconv (DepthwiseConv2D (None, 7, 7, 1152) 28800 block6b_expand_activation[0][0] __________________________________________________________________________________________________ block6b_bn (BatchNormalization) (None, 7, 7, 1152) 4608 block6b_dwconv[0][0] __________________________________________________________________________________________________ block6b_activation (Activation) (None, 7, 7, 1152) 0 block6b_bn[0][0] __________________________________________________________________________________________________ block6b_se_squeeze (GlobalAvera (None, 1152) 0 block6b_activation[0][0] __________________________________________________________________________________________________ block6b_se_reshape (Reshape) (None, 1, 1, 1152) 0 block6b_se_squeeze[0][0] __________________________________________________________________________________________________ block6b_se_reduce (Conv2D) (None, 1, 1, 48) 55344 block6b_se_reshape[0][0] __________________________________________________________________________________________________ block6b_se_expand (Conv2D) (None, 1, 1, 1152) 56448 block6b_se_reduce[0][0] __________________________________________________________________________________________________ block6b_se_excite (Multiply) (None, 7, 7, 1152) 0 block6b_activation[0][0] block6b_se_expand[0][0] __________________________________________________________________________________________________ block6b_project_conv (Conv2D) (None, 7, 7, 192) 221184 block6b_se_excite[0][0] __________________________________________________________________________________________________ block6b_project_bn (BatchNormal (None, 7, 7, 192) 768 block6b_project_conv[0][0] __________________________________________________________________________________________________ block6b_drop (Dropout) (None, 7, 7, 192) 0 block6b_project_bn[0][0] __________________________________________________________________________________________________ block6b_add (Add) (None, 7, 7, 192) 0 block6b_drop[0][0] block6a_project_bn[0][0] __________________________________________________________________________________________________ block6c_expand_conv (Conv2D) (None, 7, 7, 1152) 221184 block6b_add[0][0] __________________________________________________________________________________________________ block6c_expand_bn (BatchNormali (None, 7, 7, 1152) 4608 block6c_expand_conv[0][0] __________________________________________________________________________________________________ block6c_expand_activation (Acti (None, 7, 7, 1152) 0 block6c_expand_bn[0][0] __________________________________________________________________________________________________ block6c_dwconv (DepthwiseConv2D (None, 7, 7, 1152) 28800 block6c_expand_activation[0][0] __________________________________________________________________________________________________ block6c_bn (BatchNormalization) (None, 7, 7, 1152) 4608 block6c_dwconv[0][0] __________________________________________________________________________________________________ block6c_activation (Activation) (None, 7, 7, 1152) 0 block6c_bn[0][0] __________________________________________________________________________________________________ block6c_se_squeeze (GlobalAvera (None, 1152) 0 block6c_activation[0][0] __________________________________________________________________________________________________ block6c_se_reshape (Reshape) (None, 1, 1, 1152) 0 block6c_se_squeeze[0][0] __________________________________________________________________________________________________ block6c_se_reduce (Conv2D) (None, 1, 1, 48) 55344 block6c_se_reshape[0][0] __________________________________________________________________________________________________ block6c_se_expand (Conv2D) (None, 1, 1, 1152) 56448 block6c_se_reduce[0][0] __________________________________________________________________________________________________ block6c_se_excite (Multiply) (None, 7, 7, 1152) 0 block6c_activation[0][0] block6c_se_expand[0][0] __________________________________________________________________________________________________ block6c_project_conv (Conv2D) (None, 7, 7, 192) 221184 block6c_se_excite[0][0] __________________________________________________________________________________________________ block6c_project_bn (BatchNormal (None, 7, 7, 192) 768 block6c_project_conv[0][0] __________________________________________________________________________________________________ block6c_drop (Dropout) (None, 7, 7, 192) 0 block6c_project_bn[0][0] __________________________________________________________________________________________________ block6c_add (Add) (None, 7, 7, 192) 0 block6c_drop[0][0] block6b_add[0][0] __________________________________________________________________________________________________ block6d_expand_conv (Conv2D) (None, 7, 7, 1152) 221184 block6c_add[0][0] __________________________________________________________________________________________________ block6d_expand_bn (BatchNormali (None, 7, 7, 1152) 4608 block6d_expand_conv[0][0] __________________________________________________________________________________________________ block6d_expand_activation (Acti (None, 7, 7, 1152) 0 block6d_expand_bn[0][0] __________________________________________________________________________________________________ block6d_dwconv (DepthwiseConv2D (None, 7, 7, 1152) 28800 block6d_expand_activation[0][0] __________________________________________________________________________________________________ block6d_bn (BatchNormalization) (None, 7, 7, 1152) 4608 block6d_dwconv[0][0] __________________________________________________________________________________________________ block6d_activation (Activation) (None, 7, 7, 1152) 0 block6d_bn[0][0] __________________________________________________________________________________________________ block6d_se_squeeze (GlobalAvera (None, 1152) 0 block6d_activation[0][0] __________________________________________________________________________________________________ block6d_se_reshape (Reshape) (None, 1, 1, 1152) 0 block6d_se_squeeze[0][0] __________________________________________________________________________________________________ block6d_se_reduce (Conv2D) (None, 1, 1, 48) 55344 block6d_se_reshape[0][0] __________________________________________________________________________________________________ block6d_se_expand (Conv2D) (None, 1, 1, 1152) 56448 block6d_se_reduce[0][0] __________________________________________________________________________________________________ block6d_se_excite (Multiply) (None, 7, 7, 1152) 0 block6d_activation[0][0] block6d_se_expand[0][0] __________________________________________________________________________________________________ block6d_project_conv (Conv2D) (None, 7, 7, 192) 221184 block6d_se_excite[0][0] __________________________________________________________________________________________________ block6d_project_bn (BatchNormal (None, 7, 7, 192) 768 block6d_project_conv[0][0] __________________________________________________________________________________________________ block6d_drop (Dropout) (None, 7, 7, 192) 0 block6d_project_bn[0][0] __________________________________________________________________________________________________ block6d_add (Add) (None, 7, 7, 192) 0 block6d_drop[0][0] block6c_add[0][0] __________________________________________________________________________________________________ block7a_expand_conv (Conv2D) (None, 7, 7, 1152) 221184 block6d_add[0][0] __________________________________________________________________________________________________ block7a_expand_bn (BatchNormali (None, 7, 7, 1152) 4608 block7a_expand_conv[0][0] __________________________________________________________________________________________________ block7a_expand_activation (Acti (None, 7, 7, 1152) 0 block7a_expand_bn[0][0] __________________________________________________________________________________________________ block7a_dwconv (DepthwiseConv2D (None, 7, 7, 1152) 10368 block7a_expand_activation[0][0] __________________________________________________________________________________________________ block7a_bn (BatchNormalization) (None, 7, 7, 1152) 4608 block7a_dwconv[0][0] __________________________________________________________________________________________________ block7a_activation (Activation) (None, 7, 7, 1152) 0 block7a_bn[0][0] __________________________________________________________________________________________________ block7a_se_squeeze (GlobalAvera (None, 1152) 0 block7a_activation[0][0] __________________________________________________________________________________________________ block7a_se_reshape (Reshape) (None, 1, 1, 1152) 0 block7a_se_squeeze[0][0] __________________________________________________________________________________________________ block7a_se_reduce (Conv2D) (None, 1, 1, 48) 55344 block7a_se_reshape[0][0] __________________________________________________________________________________________________ block7a_se_expand (Conv2D) (None, 1, 1, 1152) 56448 block7a_se_reduce[0][0] __________________________________________________________________________________________________ block7a_se_excite (Multiply) (None, 7, 7, 1152) 0 block7a_activation[0][0] block7a_se_expand[0][0] __________________________________________________________________________________________________ block7a_project_conv (Conv2D) (None, 7, 7, 320) 368640 block7a_se_excite[0][0] __________________________________________________________________________________________________ block7a_project_bn (BatchNormal (None, 7, 7, 320) 1280 block7a_project_conv[0][0] __________________________________________________________________________________________________ top_conv (Conv2D) (None, 7, 7, 1280) 409600 block7a_project_bn[0][0] __________________________________________________________________________________________________ top_bn (BatchNormalization) (None, 7, 7, 1280) 5120 top_conv[0][0] __________________________________________________________________________________________________ top_activation (Activation) (None, 7, 7, 1280) 0 top_bn[0][0] __________________________________________________________________________________________________ global_average_pooling2d (Globa (None, 1280) 0 top_activation[0][0] __________________________________________________________________________________________________ flatten (Flatten) (None, 1280) 0 global_average_pooling2d[0][0] __________________________________________________________________________________________________ dense (Dense) (None, 10) 12810 flatten[0][0] ================================================================================================== Total params: 4,062,374 Trainable params: 12,810 Non-trainable params: 4,049,564 __________________________________________________________________________________________________ . 6) Train/fit model to dataset . num_epochs = 5 history = new_model.fit(train_it, validation_data = val_it, epochs=num_epochs, verbose = 1) . WARNING:tensorflow:sample_weight modes were coerced from ... to [&#39;...&#39;] WARNING:tensorflow:sample_weight modes were coerced from ... to [&#39;...&#39;] Train for 296 steps, validate for 123 steps Epoch 1/5 296/296 [==============================] - 83s 279ms/step - loss: 0.3098 - accuracy: 0.9354 - val_loss: 0.0925 - val_accuracy: 0.9786 Epoch 2/5 296/296 [==============================] - 60s 204ms/step - loss: 0.0836 - accuracy: 0.9781 - val_loss: 0.0670 - val_accuracy: 0.9829 Epoch 3/5 296/296 [==============================] - 61s 206ms/step - loss: 0.0622 - accuracy: 0.9837 - val_loss: 0.0586 - val_accuracy: 0.9811 Epoch 4/5 296/296 [==============================] - 62s 208ms/step - loss: 0.0483 - accuracy: 0.9867 - val_loss: 0.0541 - val_accuracy: 0.9845 Epoch 5/5 296/296 [==============================] - 62s 209ms/step - loss: 0.0369 - accuracy: 0.9917 - val_loss: 0.0523 - val_accuracy: 0.9839 . 7) Plot Training/Validation History . acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs = range(len(acc)) plt.subplot(121) plt.plot(epochs, acc, &#39;b&#39;, label=&#39;Training Acc&#39;) plt.plot(epochs, val_acc, &#39;r&#39;, label=&#39;Validation Acc&#39;) plt.title(&#39;Train/Val Accuracy&#39;) plt.legend() plt.subplot(122) plt.plot(epochs, loss, &#39;b&#39;, label=&#39;Training Loss&#39;) plt.plot(epochs, val_loss, &#39;r&#39;, label=&#39;Validation Loss&#39;) plt.title(&#39;Train/Val Loss&#39;) plt.legend() plt.show() .",
            "url": "https://datadote.github.io/blog/2020/03/15/transfer_learning_efficientNet.html",
            "relUrl": "/2020/03/15/transfer_learning_efficientNet.html",
            "date": " • Mar 15, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://datadote.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://datadote.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Circuit designer developing deep learning applications. Living in San Francisco bay area. Currently interested in computer vision. Tools: PyTorch, Tensorflow. .",
          "url": "https://datadote.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}