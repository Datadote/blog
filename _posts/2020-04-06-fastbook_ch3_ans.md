1. Does ethics provide a list of "right answers"?
> There is no list of right answers for ethics

2. How can working with people of different backgrounds help when considering ethical questions?
> Provide different viewpoints on how an application will affect people

3. What was the role of IBM in Nazi Germany? Why did the company participate as they did? Why did the workers participate?
> IBM provided equipment and technical assistance to classify prisoners. Company participated for money. Workers did not know better.

4. What was the role of the first person jailed in the VW diesel scandal?
> Engineer who just did what manager told him to do.

5. What was the problem with a database of suspected gang members maintained by California law enforcement officials?
> The database had many errors, but had no easy means of fixing the errors. The errors included 42 babies added to database then they were less than 1 year old. 28 of these babies were marked as admitting to being gang members.

6. Why did YouTube's recommendation algorithm recommend videos of partially clothed children to pedophiles, even although no employee at Google programmed this feature?
> The algorithm optimized itself to maximize viewer time.

7. What are the problems with the centrality of metrics?
> Centrality of metrics, to the extreme, mean one metric is being optimized. Algorithms will try to optimize the metric without thinking about negative consequences.

8. Why did Meetup.com not include gender in their recommendation system for tech meetups?
> More men than women were interested in tech meetups. If the recommendation system included gender, it might have increased this bias, and have a positive feedback loop. Recommend more to men, recommend less to women.

9. What are the six types of bias in machine learning, according to Suresh and Guttag?
> Historical bias, representation bias, measurement bias, evaluation bias, aggregation bias, deployment bias

10. Give two examples of historical race bias in the US
> Used car sales, black people offered higher initial prices. Craigslist rental-ads, Black name elicited fewer respones than a white name.

11. Where are most images in Imagenet from?
> United States and other Western countries. Eg Great Britain, Itality, Canada, Australia, Spain.

12. In the paper "Does Machine Learning Automate Moral Hazard and Error" why is sinusitis found to be predictive of a stroke?
> The data only represented people who had symptoms, went to the hospital, and got diagnosed with a stroke. Based on the data, the prediction was similar to prediction heavy utilization (propensity of people to seek cre) as well as the stroke.

13. What is representation bias?
> When there is a clear imbalance, a model will find it and amplify it or maintain it. For example in occupations, models tend to predict females as nurses, and males as doctors.

14. How are machines and people different, in terms of their use for making decisions?
> Machines can amplify bias with feedback loops very rapidly. Machines are implemented at scale without thinking of potential negative consequences, whereas people are usually screened more before being put into positions of power.

15. Is disinformation the same as "fake news"?
> No, disinformation is making people not trust one another by intertwining real and fake news together.

16. Why is disinformation through auto-generated text a particularly significant issue?
> Auto-generated disinformation scales a lot easier than human trolls. As a result, the amount of disinformation could exponentially increase without much resources.

17. What are the five ethical lenses described by the Markkula Center?
> The Rights Approach: Which option best respects the rights of all who have a stake?
> The Justice Approach: Which option treats people equally or proportionately?
> The Utilitarian Approach: Which option will produce the most good and do the least harm? 
> The Common Good Approach: Which option best serves the community as a whole, and not just some members?
> The Virtue Approach: Which option leads me to act as the sort of person I want to be?

18. Where is policy an appropriate tool for addressing data ethics issues?
> If the data ethics issues are human rights issues, then the law is an appropriate tool.
