<ol>
  <li>How is a greyscale image represented on a computer? How about a color image?
    <blockquote>
      <p>For greyscale, 1 number indicating range from white to black
For color, 3 numbers indicating intensity of red, green, blue</p>
    </blockquote>
  </li>
  <li>How are the files and folders in the <code class="highlighter-rouge">MNIST_SAMPLE</code> dataset structured? Why?
    <blockquote>
      <p>Structured similar to imagenet
Main folder -&gt; valid, train
valid,train -&gt; separate folders for each category</p>
    </blockquote>
  </li>
  <li>Explain how the “pixel similarity” approach to classifying digits works.
    <blockquote>
      <p>Pixel similarity is where you find the ideal number by averaging all the images of the “number”, then you take the mean absolute difference or mean squared difference between your input and the ideal image</p>
    </blockquote>
  </li>
  <li>What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.
    <blockquote>
      <p>List comprehension is a fast way to create a list from an iterator
a = [x for x in range(10)]; [2*x for x in a if x%2 &gt; 0]</p>
    </blockquote>
  </li>
  <li>What is a “rank 3 tensor”?
    <blockquote>
      <p>a tensor with 3 dimensions</p>
    </blockquote>
  </li>
  <li>What is the difference between tensor rank and shape? How do you get the rank from the shape?
    <blockquote>
      <p>shape gives you information on how big each axis is. Rank is length(shape)</p>
    </blockquote>
  </li>
  <li>What are RMSE and L1 norm?
    <blockquote>
      <p>RMSE and L1 norm are methods to measure difference between two things. RMSE stands for root mean squared error. L1 norm is the mean absolute difference.</p>
    </blockquote>
  </li>
  <li>How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?
    <blockquote>
      <p>Vectorize the calculations, and then use a gpu</p>
    </blockquote>
  </li>
  <li>Create a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom right 4 numbers.
    <blockquote>
      <p>a = (torch.tensor(range(9))*2).view(3,3); a[1:3, 1:3]</p>
    </blockquote>
  </li>
  <li>What is broadcasting?
    <blockquote>
      <p>broadcasting is when a math operation happens between two inputs with different shapes (usually 1 dimension), and the smaller shape automatically becomes the shape of the larger shape</p>
    </blockquote>
  </li>
  <li>Are metrics generally calculated using the training set, or the validation set? Why?
    <blockquote>
      <p>validation set in order to make sure model is generalizing and not overfitting</p>
    </blockquote>
  </li>
  <li>What is SGD?
    <blockquote>
      <p>SGD stands for stochastic gradient descent, and is a method to optimize neural network weights.</p>
    </blockquote>
  </li>
  <li>Why does SGD use mini batches?
    <blockquote>
      <p>Speeds up training, rather than going 1 example at a time</p>
    </blockquote>
  </li>
  <li>What are the 7 steps in SGD for machine learning?
    <blockquote>
      <p>Initialize parameters, forward prop, calculate loss, backprop, update weights, repeat, stop</p>
    </blockquote>
  </li>
  <li>How do we initialize the weights in a model?
    <blockquote>
      <p>Randomly. Although with certain activations, there are certain statistics we use with the random initialization. Eg ReLU -&gt; he intialization</p>
    </blockquote>
  </li>
  <li>What is “loss”?
    <blockquote>
      <p>loss is a measure the algorithm uses to optimize itself</p>
    </blockquote>
  </li>
  <li>Why can’t we always use a high learning rate?
    <blockquote>
      <p>a high learning rate could cause the loss to increase after each update</p>
    </blockquote>
  </li>
  <li>What is a “gradient”?
    <blockquote>
      <p>gradient is the slope of a function at a point</p>
    </blockquote>
  </li>
  <li>Do you need to know how to calculate gradients yourself?
    <blockquote>
      <p>If you use a framework, no. PyTorch does it for you</p>
    </blockquote>
  </li>
  <li>Why can’t we use accuracy as a loss function?
    <blockquote>
      <p>Accuracy is not fine-grain enough. Most gradients would be close to 0, and the updating would be slow</p>
    </blockquote>
  </li>
  <li>Draw the sigmoid function. What is special about its shape?
    <blockquote>
      <p>Goes between 0 and 1. 1/(1+exp(-x)). Crosses 0.5 at x=0</p>
    </blockquote>
  </li>
  <li>What is the difference between loss and metric?
    <blockquote>
      <p>Loss is the measure the algorithm uses to evaluate performance. Metric is what humans use.</p>
    </blockquote>
  </li>
  <li>What is the function to calculate new weights using a learning rate?
    <blockquote>
      <p>w = w - lr<em>params.grad, or params.data -= lr</em>params.grad</p>
    </blockquote>
  </li>
  <li>What does the <code class="highlighter-rouge">DataLoader</code> class do?
    <blockquote>
      <p>dataloader takes a list of tuples of (x,y), and batches/shuffles them for your model</p>
    </blockquote>
  </li>
  <li>Write pseudo-code showing the basic steps taken each epoch for SGD.
    <blockquote>
      <p>forward prop, calc loss, backprop, update weight
pred = model(x)
loss = loss_func(pred, y)
loss.backward()
p -= p.grad*lr
p.grad.zero_()</p>
    </blockquote>
  </li>
  <li>Create a function which, if passed two arguments <code class="highlighter-rouge">[1,2,3,4]</code> and <code class="highlighter-rouge">'abcd'</code>, returns <code class="highlighter-rouge">[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]</code>. What is special about that output data structure?
    <blockquote>
      <p>[x for x in zip([1,2,3,4], ‘abcd’)]</p>
    </blockquote>
  </li>
  <li>What does <code class="highlighter-rouge">view</code> do in PyTorch?
    <blockquote>
      <p>view does reshaping</p>
    </blockquote>
  </li>
  <li>What are the “bias” parameters in a neural network? Why do we need them?
    <blockquote>
      <p>bias parameters are randomly initialized parameters used in the forward prop equation. y = w*x + b. B lets the equation center itself.</p>
    </blockquote>
  </li>
  <li>What does the <code class="highlighter-rouge">@</code> operator do in python?
    <blockquote>
      <p>@ = np.matmul = element-wise matrix multiplication</p>
    </blockquote>
  </li>
  <li>What does the <code class="highlighter-rouge">backward</code> method do?
    <blockquote>
      <p>backward calculates the gradients from the given point
eg loss.backward()</p>
    </blockquote>
  </li>
  <li>Why do we have to zero the gradients?
    <blockquote>
      <p>so that the update is reflective of the current step, and not of previous steps
loss.backward() accumulates gradients in each function</p>
    </blockquote>
  </li>
  <li>What information do we have to pass to <code class="highlighter-rouge">Learner</code>?
    <blockquote>
      <p>Learner(dls, net, metrics)
Learner(dls, net, opt_func, loss_func, metrics)</p>
    </blockquote>
  </li>
  <li>Show python or pseudo-code for the basic steps of a training loop.
    <blockquote>
      <p>for i in range(epochs):
for xb,yb in dl:
calc_grad(xb, yb, model)
p.data -= p.grad.data*lr
p.grad.zero_()</p>
    </blockquote>
  </li>
  <li>What is “ReLU”? Draw a plot of it for values from <code class="highlighter-rouge">-2</code> to <code class="highlighter-rouge">+2</code>.
    <blockquote>
      <p>relu(x) = max(x,0). Relu is 0 for x 0 or less, and = x for x &gt; 0</p>
    </blockquote>
  </li>
  <li>What is an “activation function”?
    <blockquote>
      <p>activation function is a nonlinearity applied to the linear y=wx+b equation</p>
    </blockquote>
  </li>
  <li>What’s the difference between <code class="highlighter-rouge">F.relu</code> and <code class="highlighter-rouge">nn.ReLU</code>?
    <blockquote>
      <p>F.relu is the function.
nn.Relu is a PyTorch module (layer) that does the same thing.</p>
    </blockquote>
  </li>
  <li>The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?
    <blockquote>
      <p>We use more nonlinearities, which means more layers, for better performance. For the same performance, deeper networks tend to need less memory/calculations.</p>
    </blockquote>
  </li>
</ol>
